# ADR-001: Выбор архитектуры модуля аналитики

## Статус
Принято

## Контекст
Существует интернет-магазин с оборотом ~200 млн, генерирующий поток событий о поведении пользователей (просмотры, добавления в корзину, покупки, различные метрики).  
Требуется спроектировать модуль аналитики, который:

1. В реальном времени собирает эти события, **не влияя на производительность основного OLTP-сайта**.  
2. Обеспечивает доступность аналитических данных для построения дашбордов и отчётов (срезы по часам/дням, история, метрики).  
3. Позволяет аналитикам видеть ключевые события (особенно **покупки**) практически в реальном времени. Допустима задержка до **15 минут**, но желательно меньше.  
4. Поддерживает повышенную нагрузку в пиковые периоды (например, перед праздниками).  
5. Используется **только внутри компании**, данные не передаются наружу.  
6. В дальнейшем данные будут использоваться для **автоматизированного изменения цен в зависимости от спроса**.  

Основное противоречие заключается в конфликте архитектурных характеристик:

* **Для OLTP-системы (сайт):** производительность, низкая задержка, отказоустойчивость.  
* **Для OLAP-системы (аналитика):** высокая "записоемкость", гибкость схемы данных, возможность агрегирующих запросов.  

## Варианты решения

### Вариант A: Прямая запись в аналитическую БД
События пишутся напрямую в OLAP-хранилище (например, ClickHouse или PostgreSQL с расширениями).

* **Плюсы:** простая архитектура, данные сразу доступны для анализа.  
* **Минусы:** риск нагрузки на хранилище, сайт ждёт ответа, возможны задержки при пиках. Нет буферизации.

### Вариант B: Асинхронная отправка в брокер сообщений (Kafka)
События с сайта отправляются в Kafka в формате JSON.  
Консьюмеры читают поток, обогащают данные и загружают их в аналитическое хранилище.  
Появляется возможность модульной обработки данных (фильтрация, агрегация, enrichment).

* **Плюсы:**  
  - Развязка OLTP и OLAP.  
  - Масштабируемость (партиции Kafka, консьюмеры).  
  - Буферизация при пиках (например, праздники).  
  - Поддержка near real-time (задержки до секунд-минут).  
  - Гибкость: можно строить разные пайплайны (дашборды, рекомендательные системы, автоматизация цен).  

* **Минусы:**  
  - Более высокая сложность, чем у прямой записи.  
  - Требуется поддержка Kafka-кластера (но можно использовать минимальную конфигурацию или managed-версию).  

### Вариант C: CDC (Change Data Capture) из OLTP-БД
Использование инструментов CDC (например, Debezium), читающих WAL-журналы и передающих изменения в OLAP.

* **Плюсы:** не требует изменений в коде сайта.  
* **Минусы:** жёсткая связь со схемой OLTP-БД, захватывает только изменения в базе, не все бизнес-события. Риск нагрузки на основную БД.  

## Решение
Выбран **Вариант B: Асинхронная отправка событий через Kafka (CQRS-паттерн)**.

### Почему:
1. **Масштабируемость:** Kafka позволяет обрабатывать нагрузку даже при обороте 200 млн и росте спроса в праздники.  
2. **Доступность:** при сбое аналитического хранилища данные сохраняются в Kafka и будут загружены после восстановления.  
3. **Гибкость:** можно строить разные конвейеры обработки событий без влияния на работу сайта.  
4. **Согласованность:** выбрана модель eventual consistency. Данные в аналитике будут появляться с задержкой (обычно секунды–минуты), что соответствует требованиям бизнеса (<= 15 минут).  
5. **Модульность и адаптивность:** консьюмеры можно подключать/отключать, расширяя систему новыми функциями.  

## Последствия

### Положительные
* **Масштабируемость:** Kafka и консьюмеры масштабируются независимо.  
* **Буферизация:** выдерживает пики нагрузки.  
* **Отказоустойчивость:** при падении аналитики данные не теряются. Можно настроить минимальное дублирование брокеров и тестирование отказов.  
* **Гибкость:** возможность адаптации под новые метрики (например, автоматическая корректировка цен).  
* **Производительность:** сайт не блокируется при аналитических запросах.  

### Отрицательные
* **Сложность:** распределённая архитектура, выше порог поддержки.  
* **Согласованность в конечном счёте:** аналитика отображает данные с задержкой, до 15 минут.  
* **Операционные затраты:** сопровождение Kafka-кластера.  
* **Дублирование данных:** OLTP-БД и аналитическое хранилище. Нужно следить за консистентностью.  

---

## Нефункциональные требования и фитнес-функции

### 1. Свежесть данных (end-to-end latency)
- **Метрика:** время от генерации события на сайте до его появления в дашборде.  
- **Порог:**  
  - обычный режим: P95 <= 5 мин, P99 <= 10 мин;  
  - пиковая нагрузка (праздники): P95 <= 10 мин, P99 <= 15 мин.  
- **Проверка:** синтетическая нагрузка + замер lag в Kafka и появления в OLAP.

### 2. Пропускная способность
- **Метрика:** событий/сек.  
- **Порог:** выдержка >= 5× среднесуточной нагрузки без потерь данных.  
- **Проверка:** нагрузочное тестирование (k6, kafka-producer-perf-test).

### 3. Надёжность доставки
- **Метрика:** % потерянных и дублированных событий.  
- **Порог:** потери <= 0.001%, дубли <= 0.01%.  
- **Проверка:** дедупликация по UUID, fault-injection (отключение консьюмеров, сбои сети).

### 4. Доступность
- **Метрика:** uptime по компонентам.  
- **Порог:**  
  - ingest >= 99.5%;  
  - Kafka >= 99.9%;  
  - дашборды >= 99.0%.  
- **Проверка:** health-checks, uptime monitoring.

### 5. Время восстановления (RTO/RPO)
- **Метрика:** RTO и RPO.  
- **Порог:** RTO <= 30 минут; RPO = 0 (Kafka хранит буфер ≥ 7 дней).  
- **Проверка:** регулярные DR-учения (отключение OLAP, сбои брокеров).

### 6. Масштабируемость
- **Метрика:** рост throughput при добавлении ресурсов.  
- **Порог:** >= 60–80% линейного роста.  
- **Проверка:** тесты с добавлением партиций и консьюмеров.

### 7. Обновление витрин (batch)
- **Метрика:** длительность hourly и daily задач.  
- **Порог:** hourly <= 5 минут, daily <= 30 минут.  
- **Проверка:** замер времени выполнения оркестратором (Airflow/cron).

---